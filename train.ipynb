{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f43552",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6928d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,\\\n",
    "                         TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "159bf124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset):\n",
    "    for i in range(5):\n",
    "        print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c30d73",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23dd034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"shuttie/dadjokes\")\n",
    "train_data = ds[\"train\"]\n",
    "test_data = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe365ad",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "<p>GPT-like models expect casual language modeling as all-in-one input containing all the text at once, since our dataset has questions separated from their responses, so we need to combine them as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e97a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fields(record):\n",
    "    q = record.get(\"question\")\n",
    "    r = record.get(\"response\")\n",
    "    if q is None or r is None:\n",
    "        return {\"text\": \"\"}\n",
    "    return {\"text\": q.strip() + \" \" + r.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I asked my priest how he gets holy water He said it’s just regular water, he just boils the hell out of it'}\n",
      "{'text': 'Life Hack: If you play My Chemical Romance loud enough in your yard your grass will cut itself'}\n",
      "{'text': 'OMG. SISTERS. JAMES. CHARLES. IS. DOING. A GIVEAWAY his career'}\n",
      "{'text': 'Why did Mr.  Potato Head get pulled over He was baked'}\n",
      "{'text': 'On zombie cravings.  My kids and i had some fun with these on a car trip this past weekend.   What do zombie plumbers crave.  Draaaaains.   What do zombie pilots crave.  Planes.  Plaaaanes.   What do zombie conductors crave.  Traaaains.   What do zombie opthalmologists crave.  Fraaames.   What do zombie construction workers crave.  Craaanes.   What do zombie nurses crave.  Paaains.   What do vampires crave Blood'}\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(combine_fields)\n",
    "test_data = test_data.map(combine_fields)\n",
    "\n",
    "train_data = train_data.remove_columns([\"question\",\"response\"])\n",
    "test_data = test_data.remove_columns([\"question\",\"response\"])\n",
    "\n",
    "sample_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e82f87",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "<p>is the process of breaking down documents or inputs of text into small units called <strong>tokens</strong>, in order to make it easier for the machine to process the text.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b6bcbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Special Topics\\Code\\Assignment1\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08be45",
   "metadata": {},
   "source": [
    "### Example of a sentence after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f9be9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5195, 750, 262, 9015, 3272, 262, 2975, 30, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Why did the chicken cross the road?\", padding=\"max_length\", max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd6a9d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52000/52000 [00:01<00:00, 33896.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I asked my priest how he gets holy water He said it’s just regular water, he just boils the hell out of it', 'input_ids': [40, 1965, 616, 11503, 703, 339, 3011, 11386, 1660, 679, 531, 340, 447, 247, 82, 655, 3218, 1660, 11, 339, 655, 40169, 262, 5968, 503, 286, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': 'Life Hack: If you play My Chemical Romance loud enough in your yard your grass will cut itself', 'input_ids': [14662, 18281, 25, 1002, 345, 711, 2011, 24872, 36555, 7812, 1576, 287, 534, 12699, 534, 8701, 481, 2005, 2346, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': 'OMG. SISTERS. JAMES. CHARLES. IS. DOING. A GIVEAWAY his career', 'input_ids': [2662, 38, 13, 311, 8808, 4877, 13, 449, 29559, 13, 28521, 28378, 13, 3180, 13, 8410, 2751, 13, 317, 402, 9306, 12298, 4792, 465, 3451, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': 'Why did Mr.  Potato Head get pulled over He was baked', 'input_ids': [5195, 750, 1770, 13, 220, 43876, 7123, 651, 5954, 625, 679, 373, 22979, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': 'On zombie cravings.  My kids and i had some fun with these on a car trip this past weekend.   What do zombie plumbers crave.  Draaaaains.   What do zombie pilots crave.  Planes.  Plaaaanes.   What do zombie conductors crave.  Traaaains.   What do zombie opthalmologists crave.  Fraaames.   What do zombie construction workers crave.  Craaanes.   What do zombie nurses crave.  Paaains.   What do vampires crave Blood', 'input_ids': [2202, 15956, 269, 42335, 13, 220, 2011, 3988, 290, 1312, 550, 617, 1257, 351, 777, 319, 257, 1097, 5296, 428, 1613, 5041, 13, 220, 220, 1867, 466, 15956, 458, 17024, 44434, 13, 220, 12458, 46071, 1299, 13, 220, 220, 1867, 466, 15956, 14982, 44434, 13, 220, 42788, 13, 220, 1345, 46071, 7305, 13, 220, 220, 1867, 466, 15956, 3189, 669, 44434, 13, 220, 4759], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(record):\n",
    "    return tokenizer(record[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_data = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "sample_data(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ac3ee",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bb3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Special Topics\\Code\\Assignment1\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='./joke-model',\n",
    "    num_train_epochs=5,\n",
    "    # per_device_eval_batch_size=8, if we add evaluation later or split the dataset that we have\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4412abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a17efe",
   "metadata": {},
   "source": [
    "## Trying the model (Before vs. After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Before Fine-tuning:\n",
      " Why did the chicken cross the road?”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " After Fine-tuning:\n",
      " Why did the chicken cross the road?  To get to the other side of the road.                               \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "prompt = \"Why did the chicken cross the road?\"\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "original_output = original_model.generate(**inputs, max_length=64)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./joke-model/checkpoint-65000\")\n",
    "fine_tuned_output = fine_tuned_model.generate(**inputs, max_length=64)\n",
    "fine_tuned_text = tokenizer.decode(fine_tuned_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\" Before Fine-tuning:\\n\", original_text)\n",
    "print(\"\\n After Fine-tuning:\\n\", fine_tuned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185f0cb",
   "metadata": {},
   "source": [
    "### Explanation of the output above:\n",
    "<ul>\n",
    "<i>Before fine-tuning:<br>\n",
    "The original pre-trained model (distilgpt2) is not tuned to generate jokes, so instead it fills the rest with eos padding, that's why it's all empty. Basically, GPT-2 didn’t know what to say. It padded silence because it's in its pretrained state.</i>\n",
    "<br>\n",
    "<br>\n",
    "<i>After fine-tuning:<br>\n",
    "After using the jokes dataset, the model became able to understand the joke prompt and then generate a proper answer as a continue or punchline, thanks to fine-tuning it on the jokes.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f8773",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "<p>Evaluating the model using the test set (unseen data), but first we need to tokenize it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3028d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [00:00<00:00, 24373.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23035208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 76.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.435093402862549, 'eval_runtime': 2.3533, 'eval_samples_per_second': 594.913, 'eval_steps_per_second': 74.364}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./joke-model/checkpoint-65000/\")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='./joke-model',\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34776aa",
   "metadata": {},
   "source": [
    "<p>These results make perfect sense, while a loss of 3.435 is usually considered very high, but since we're tuning the model on a small dataset these results are considered decent. A training set of 52K record of jokes is a very small dataset in the world of NLP, and here's a demonstration that shows how small this dataset is:</p>\n",
    "<p>Assuming ~20 tokens per joke → ~1 million tokens for the whole dataset.\n",
    "That’s ~0.0125% of what GPT-2 was trained on which is <a href=\"https://lambda.ai/blog/demystifying-gpt-3\">~40GB of Internet text (8 million documents) which is ~10 billion tokens</a> 😅.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62e24b",
   "metadata": {},
   "source": [
    "## GPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d18f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())# True means that there is at least one GPU|TPU.\n",
    "print(torch.cuda.device_count())# Shows how many devices (GPUs | TPUs) are there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
